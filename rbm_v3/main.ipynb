{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms\n",
    "\n",
    "\n",
    "base_dir = '/Users/yishai/Desktop/GenerativeTabularFusion'\n",
    "import os\n",
    "os.chdir(base_dir)    \n",
    "from app.dotenv import base_dir, data_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "n_epochs: 10\n",
      "lr: 0.01\n",
      "n_hid: 128\n",
      "n_vis: 784\n"
     ]
    }
   ],
   "source": [
    "########## CONFIGURATION ##########\n",
    "# batch_size = 64\n",
    "# VISIBLE_UNITS = 784  # 28 x 28 images\n",
    "# HIDDEN_UNITS = 128\n",
    "CD_K = 2\n",
    "# EPOCHS = 10\n",
    "\n",
    "# DATA_FOLDER = 'data/mnist'\n",
    "batch_size = int(os.getenv('batch_size'))# batch size\n",
    "n_epochs = int(os.getenv('n_epochs')) # number of epochs\n",
    "lr = float(os.getenv('lr')) # learning rate\n",
    "n_hid = int(os.getenv('n_hid')) # number of neurons in the hidden layer\n",
    "n_vis = int(os.getenv('n_vis')) # input size. i.e number of neurons in the visible layer\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'n_epochs: {n_epochs}')\n",
    "print(f'lr: {lr}')\n",
    "print(f'n_hid: {n_hid}')\n",
    "print(f'n_vis: {n_vis}')\n",
    "CUDA = torch.cuda.is_available()\n",
    "CUDA_DEVICE = 0\n",
    "\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "train size:  60000 test size:  10000\n",
      "batch size: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "########## LOADING DATASET ##########\n",
    "print('Loading dataset...')\n",
    "\n",
    "from app.load_data.MNIST import data_loaders\n",
    "train_loader, test_loader = data_loaders(batch_size,data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Error (epoch=0): 5792716.5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CUDA:\n\u001b[1;32m     19\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 21\u001b[0m     batch_error \u001b[38;5;241m=\u001b[39m \u001b[43mrbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrastive_divergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     epoch_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_error\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch Error (epoch=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, epoch_error))\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/app/rbm/model.py:183\u001b[0m, in \u001b[0;36mRBM_v3.contrastive_divergence\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[1;32m    182\u001b[0m     visible_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_visible(hidden_activations)\n\u001b[0;32m--> 183\u001b[0m     hidden_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible_probabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     hidden_activations \u001b[38;5;241m=\u001b[39m (hidden_probabilities \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_probabilities(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hidden))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    186\u001b[0m negative_visible_probabilities \u001b[38;5;241m=\u001b[39m visible_probabilities\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/app/rbm/model.py:164\u001b[0m, in \u001b[0;36mRBM_v3.sample_hidden\u001b[0;34m(self, visible_probabilities)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_hidden\u001b[39m(\u001b[38;5;28mself\u001b[39m, visible_probabilities):\n\u001b[1;32m    163\u001b[0m     hidden_activations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(visible_probabilities, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_bias\n\u001b[0;32m--> 164\u001b[0m     hidden_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_probabilities\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/app/rbm/model.py:215\u001b[0m, in \u001b[0;36mRBM_v3._sigmoid\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sigmoid\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########## TRAINING RBM ##########\n",
    "print('Training RBM...')\n",
    "# from rbm import RBM\n",
    "\n",
    "from app.rbm.model import RBM_v3\n",
    "\n",
    "RBM = RBM_v3\n",
    "# print(RBM)    \n",
    "\n",
    "rbm = RBM(n_vis, n_hid, CD_K, use_cuda=CUDA)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_error = 0.0\n",
    "\n",
    "    for batch, _ in train_loader:\n",
    "        batch = batch.view(len(batch), n_vis)  # flatten input data\n",
    "\n",
    "        if CUDA:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        batch_error = rbm.contrastive_divergence(batch)\n",
    "\n",
    "        epoch_error += batch_error\n",
    "\n",
    "    print('Epoch Error (epoch=%d): %.4f' % (epoch, epoch_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    }
   ],
   "source": [
    "########## EXTRACT FEATURES ##########\n",
    "print('Extracting features...')\n",
    "\n",
    "train_features = np.zeros((len(train_loader.dataset), n_hid))\n",
    "train_labels = np.zeros(len(train_loader.dataset))\n",
    "test_features = np.zeros((len(test_loader.dataset), n_hid))\n",
    "test_labels = np.zeros(len(test_loader.dataset))\n",
    "\n",
    "for i, (batch, labels) in enumerate(train_loader):\n",
    "    batch = batch.view(len(batch), n_vis)  # flatten input data\n",
    "\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "\n",
    "    train_features[i*batch_size:i*batch_size+len(batch)] = rbm.sample_hidden(batch).cpu().numpy()\n",
    "    train_labels[i*batch_size:i*batch_size+len(batch)] = labels.numpy()\n",
    "\n",
    "for i, (batch, labels) in enumerate(test_loader):\n",
    "    batch = batch.view(len(batch), n_vis)  # flatten input data\n",
    "\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "\n",
    "    test_features[i*batch_size:i*batch_size+len(batch)] = rbm.sample_hidden(batch).cpu().numpy()\n",
    "    test_labels[i*batch_size:i*batch_size+len(batch)] = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yishai/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 9314/10000;0\n",
      "Result: 9314/10000;0.931400\n",
      "Result: 9314/10000;0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########## CLASSIFICATION ##########\n",
    "print('Classifying...')\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_features, train_labels)\n",
    "predictions = clf.predict(test_features)\n",
    "\n",
    "print('Result: %d/%d;%f' % (sum(predictions == test_labels), test_labels.shape[0], sum(predictions == test_labels)/test_labels.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
