{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, num_samples, n_targets: int = 8, random_n_targets=True, nu=None, scnr=None):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.n_targets = n_targets\n",
    "        self.random_n_targets = random_n_targets\n",
    "        self.with_targets = n_targets > 0\n",
    "        self.scnr = scnr\n",
    "        self.nu = torch.tensor([nu]) if nu is not None else None\n",
    "\n",
    "        # Parameters\n",
    "        self.N = 64  # Samples per pulse (fast-time)\n",
    "        self.K = 64  # Pulses per frame (slow-time)\n",
    "        self.B = 50e6  # Chirp bandwidth (Hz)\n",
    "        self.T0 = 1e-3  # PRI (s)\n",
    "        self.fc = 9.39e9  # Carrier frequency (Hz)\n",
    "        self.c = 3e8  # Speed of light (m/s)\n",
    "        self.CNR = 15  # Clutter-to-noise ratio (dB)\n",
    "\n",
    "        # Range and Doppler parameters\n",
    "        self.r_min, self.r_max = 0, 189  # Range interval (m)\n",
    "        self.v_min, self.v_max = -7.8, 7.8  # Doppler interval (m/s)\n",
    "        self.vc_min, self.vc_max = -7.8, 7.8  # Clutter min/max velocity (m/s)\n",
    "        self.dr = 3   # Range resolution (m)\n",
    "        self.dv = 0.249  # Doppler resolution (m/s)\n",
    "\n",
    "        # Calculate range and Doppler bins\n",
    "        self.R = torch.arange(self.r_min, self.r_max + self.dr, self.dr)\n",
    "        self.V = torch.arange(self.v_min, self.v_max + self.dv, self.dv)\n",
    "        self.dR = len(self.R)  # Number of range bins\n",
    "        self.dV = len(self.V)  # Number of Doppler bins\n",
    "\n",
    "        # Noise power calculation\n",
    "        self.sigma2 = self.N / (2 * 10 ** (self.CNR / 10))\n",
    "        self.cn_norm = torch.sqrt(torch.tensor(self.N * self.K * (self.N // 2 + self.sigma2)))\n",
    "\n",
    "        # Precompute the range steering matrix for matched filtering.\n",
    "        w_range = (2 * torch.pi * (2 * self.B) / (self.c * self.N)) * self.R.unsqueeze(-1) * torch.arange(self.N)\n",
    "        self.range_steering_full = torch.exp(-1j * w_range)\n",
    "\n",
    "    def generate_target_signal(self, ranges, velocities, phases, SCNR_dBs):\n",
    "        w_r = (2 * torch.pi * 2 * self.B * ranges) / (self.c * self.N)\n",
    "        range_steering = torch.exp(-1j * torch.outer(w_r, torch.arange(self.N)))\n",
    "        w_d = (2 * torch.pi * self.T0 * 2 * self.fc * velocities) / self.c\n",
    "        doppler_steering = torch.exp(-1j * torch.outer(w_d, torch.arange(self.K)))\n",
    "        rd_signal = range_steering.unsqueeze(-1) * doppler_steering.unsqueeze(1)\n",
    "        rd_signal = rd_signal * torch.exp(1j * phases)\n",
    "        S_norm = torch.linalg.norm(rd_signal, dim=(1, 2)).real\n",
    "        sig_amp = (10 ** (SCNR_dBs / 20)) * (self.cn_norm / S_norm)\n",
    "        rd_signal = (sig_amp.unsqueeze(-1).unsqueeze(-1) * rd_signal).sum(dim=0)\n",
    "        return rd_signal\n",
    "\n",
    "    def generate_clutter(self, nu):\n",
    "        # random clutter velocity\n",
    "        clutter_vel = torch.empty(1).uniform_(self.vc_min, self.vc_max)\n",
    "        fd = (2 * torch.pi * (2 * self.fc * clutter_vel) / self.c)\n",
    "        sigma_f = 0.05\n",
    "        p, q = torch.meshgrid(torch.arange(self.N), torch.arange(self.K), indexing='ij')\n",
    "        M = torch.exp(-2 * torch.pi ** 2 * sigma_f ** 2 * (p - q) ** 2 - 1j * (p - q) * fd * self.T0)\n",
    "\n",
    "        # complex random generation\n",
    "        z = torch.randn(self.K, self.dR, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2.0))\n",
    "        e, V = torch.linalg.eigh(M)\n",
    "        e_sqrt = torch.sqrt(torch.maximum(e.real, torch.tensor(0.0)))\n",
    "        E = torch.diag(e_sqrt)\n",
    "        A = V @ E.to(V.dtype)\n",
    "        w_t = A @ z\n",
    "\n",
    "        # gamma-distributed scattered reflectivities\n",
    "        s = torch.distributions.Gamma(nu, nu).sample((self.dR,))\n",
    "        c_t = (torch.sqrt(s).unsqueeze(0) * w_t.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # range steering\n",
    "        c_r_steer = torch.exp(-1j * 2 * torch.pi * torch.outer(torch.arange(self.N), self.R) * (2 * self.B) / (self.c * self.N))\n",
    "        C = c_r_steer @ c_t.transpose(0, 1)\n",
    "        return C\n",
    "\n",
    "    def gen_frame_and_labels(self):\n",
    "        W = (torch.randn(self.N, self.K, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2.0 * self.sigma2)))\n",
    "        nu = torch.empty(1).uniform_(0.1, 1.5) if self.nu is None else self.nu\n",
    "        C = self.generate_clutter(nu)\n",
    "        S = torch.zeros_like(W)\n",
    "        rd_label = torch.zeros(self.dR, self.dV)\n",
    "\n",
    "        if self.with_targets:\n",
    "            n = torch.randint(1, self.n_targets + 1, (1,)) if self.random_n_targets else self.n_targets\n",
    "            ranges = torch.empty(n).uniform_(self.r_min, self.r_max)\n",
    "            velocities = torch.empty(n).uniform_(self.v_min, self.v_max)\n",
    "            phases = torch.empty(n, 1, 1).uniform_(0, 2 * torch.pi)\n",
    "            SCNR_dBs = torch.empty(n).uniform_(-20, 10) if self.scnr is None else self.scnr * torch.ones(n)\n",
    "            S = self.generate_target_signal(ranges, velocities, phases, SCNR_dBs)\n",
    "\n",
    "            # Build the label\n",
    "            for r, v in zip(ranges, velocities):\n",
    "                r_bin = torch.argmin(torch.abs(self.R - r))\n",
    "                v_bin = torch.argmin(torch.abs(self.V - v))\n",
    "                rd_label[r_bin, v_bin] = 1\n",
    "\n",
    "        return S, C, W, rd_label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        S, C, W, rd_label = self.gen_frame_and_labels()\n",
    "        X = S + C + W\n",
    "\n",
    "        # Matched filtering in range dimension:\n",
    "        X_range = self.range_steering_full.conj() @ X\n",
    "        S_range = self.range_steering_full.conj() @ S\n",
    "\n",
    "        S_RD = torch.fft.fft(S_range, dim=1)\n",
    "        S_RD = torch.fft.fftshift(S_RD, dim=1)\n",
    "        # Use the real part for the clean map\n",
    "        clean_RD_map = S_RD.flip(dims=[1]).real\n",
    "\n",
    "        X_RD = torch.fft.fft(X_range, dim=1)\n",
    "        X_RD = torch.fft.fftshift(X_RD, dim=1)\n",
    "        # Use the magnitude for the noisy version\n",
    "        noisy_RD_map = X_RD.abs().flip(dims=[1])\n",
    "        \n",
    "        return {\n",
    "            'clean': abs(S),\n",
    "            'noisy': abs(X),\n",
    "            'rd_label': rd_label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_with_targets = RadarDataset(num_samples=10240, n_targets=8, random_n_targets=True, scnr=-10)\n",
    "train_dataset_with_targets[0][\"clean\"].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        emb = x[:, None] * emb[None, :]  # shape: (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb  # shape: (B, dim)\n",
    "    \n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1)\n",
    "        self.proj_out = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x)\n",
    "        qkv = self.qkv(h)  # (B, 3C, H, W)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=1)\n",
    "\n",
    "        # reshape to (B, num_heads, C//num_heads, H*W)\n",
    "        q = q.reshape(B, self.num_heads, C // self.num_heads, H * W)\n",
    "        k = k.reshape(B, self.num_heads, C // self.num_heads, H * W)\n",
    "        v = v.reshape(B, self.num_heads, C // self.num_heads, H * W)\n",
    "\n",
    "        attn = torch.einsum('bhcn,bhcm->bhnm', q, k) / math.sqrt(C // self.num_heads)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        out = torch.einsum('bhnm,bhcm->bhcn', attn, v)\n",
    "        out = out.reshape(B, C, H, W)\n",
    "        out = self.proj_out(out)\n",
    "        return x + out\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=out_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=out_ch),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    def forward(self, x):\n",
    "        x_conv = self.conv(x)\n",
    "        x_down = self.pool(x_conv)\n",
    "        return x_conv, x_down\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(out_channels + skip_channels, out_channels)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.size() != skip.size():\n",
    "            diffY = skip.size()[2] - x.size()[2]\n",
    "            diffX = skip.size()[3] - x.size()[3]\n",
    "            x = F.pad(x, [0, diffX, 0, diffY])\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "    \n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1, time_emb_dim=32):\n",
    "        \"\"\"\n",
    "        in_channels=2 because we concatenate:\n",
    "            - x_t: the noised (or current reverse process) image (1 channel)\n",
    "            - cond: the conditioning (noisy observed) image (1 channel)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.time_emb = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 256)\n",
    "        # Bottleneck with attention\n",
    "        self.bot = DoubleConv(256, 512)\n",
    "        self.attn = SelfAttention2d(512)\n",
    "        # Decoder with time embedding injections\n",
    "        self.up1 = Up(512, skip_channels=256, out_channels=256)\n",
    "        self.up2 = Up(256, skip_channels=256, out_channels=256)\n",
    "        self.up3 = Up(256, skip_channels=128, out_channels=128)\n",
    "        self.up4 = Up(128, skip_channels=64, out_channels=64)\n",
    "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        # Time embeddings injected at various stages:\n",
    "        self.time_proj_bot = nn.Linear(time_emb_dim, 512)\n",
    "        self.time_proj_up1 = nn.Linear(time_emb_dim, 256)\n",
    "        self.time_proj_up2 = nn.Linear(time_emb_dim, 256)\n",
    "        self.time_proj_up3 = nn.Linear(time_emb_dim, 128)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B,2,H,W) where channels are [x_t, cond]\n",
    "        t: (B,) normalized timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_emb(t)  # shape: (B, time_emb_dim)\n",
    "\n",
    "        x1 = self.inc(x)                 # (B,64,H,W)\n",
    "        x2_skip, x2 = self.down1(x1)     # (B,128,...)\n",
    "        x3_skip, x3 = self.down2(x2)     # (B,256,...)\n",
    "        x4_skip, x4 = self.down3(x3)     # (B,256,...)\n",
    "        \n",
    "        x_bot = self.bot(x4)\n",
    "        x_bot = self.attn(x_bot)\n",
    "        t_bot = self.time_proj_bot(t_emb).view(-1, 512, 1, 1)\n",
    "        x_bot = x_bot + t_bot\n",
    "        \n",
    "        x = self.up1(x_bot, x4_skip)\n",
    "        t_up1 = self.time_proj_up1(t_emb).view(-1, 256, 1, 1)\n",
    "        x = x + t_up1\n",
    "        \n",
    "        x = self.up2(x, x3_skip)\n",
    "        t_up2 = self.time_proj_up2(t_emb).view(-1, 256, 1, 1)\n",
    "        x = x + t_up2\n",
    "        \n",
    "        x = self.up3(x, x2_skip)\n",
    "        t_up3 = self.time_proj_up3(t_emb).view(-1, 128, 1, 1)\n",
    "        x = x + t_up3\n",
    "        \n",
    "        x = self.up4(x, x1)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDiffusion(nn.Module):\n",
    "    def __init__(self, model, T=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        self.model = model  # ConditionalUNet\n",
    "        self.T = T\n",
    "        self.register_buffer(\"betas\", torch.linspace(beta_start, beta_end, T))\n",
    "        self.register_buffer(\"alphas\", 1.0 - self.betas)\n",
    "        self.register_buffer(\"alpha_bars\", torch.cumprod(self.alphas, dim=0))\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: add noise to x0 at timestep t.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar = self.alpha_bars[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar = (1 - self.alpha_bars[t]).sqrt().view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "    def p_losses(self, x0, t, cond):\n",
    "        \"\"\"\n",
    "        Loss: train network to predict the noise added.\n",
    "        x0: clean image (B,1,H,W)\n",
    "        cond: conditioning (observed noisy image) (B,1,H,W)\n",
    "        \"\"\"\n",
    "        x_noisy, noise = self.q_sample(x0, t)\n",
    "        t_norm = t.float() / self.T\n",
    "        \n",
    "        # Concatenate along channel dimension: [x_noisy, cond]\n",
    "        model_input = torch.cat([x_noisy, cond], dim=1)\n",
    "        noise_pred = self.model(model_input, t_norm)\n",
    "        return F.mse_loss(noise_pred, noise)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, cond):\n",
    "        \"\"\"\n",
    "        One reverse diffusion step (from x_t to x_{t-1}).\n",
    "        \"\"\"\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        alphas_t = self.alphas[t].view(-1, 1, 1, 1)\n",
    "        alpha_bars_t = self.alpha_bars[t].view(-1, 1, 1, 1)\n",
    "        t_norm = (torch.tensor([t], device=x.device).float() / self.T).repeat(x.shape[0])\n",
    "\n",
    "        model_input = torch.cat([x, cond], dim=1)\n",
    "        noise_pred = self.model(model_input, t_norm)\n",
    "\n",
    "        coef1 = 1 / torch.sqrt(alphas_t)\n",
    "        coef2 = betas_t / torch.sqrt(1 - alpha_bars_t)\n",
    "        mean = coef1 * (x - coef2 * noise_pred)\n",
    "        \n",
    "        noise = torch.randn_like(x) if t > 0 else 0\n",
    "        return mean + torch.sqrt(betas_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, shape):\n",
    "        \"\"\"\n",
    "        Generate a denoised image conditioned on cond.\n",
    "        cond: (B,1,H,W) the observed noisy image.\n",
    "        shape: desired shape of x (B,1,H,W)\n",
    "        \"\"\"\n",
    "        x = torch.randn(shape, device=cond.device)\n",
    "        for t in reversed(range(self.T)):\n",
    "            t_tensor = torch.tensor([t], device=x.device)\n",
    "            x = self.p_sample(x, t_tensor, cond)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'RadarDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'RadarDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 22841) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:947\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 947\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 22841) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 85\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond_diffusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#     val_loss, gen_mse, gen_psnr = validate(cond_diffusion, val_loader, device)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#     train_losses.append(train_loss)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#     plt.legend()\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     plt.show()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(diffusion, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m diffusion\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# (B,H,W)\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnoisy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (B,H,W)\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 22841) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(diffusion, dataloader, optimizer, device):\n",
    "    diffusion.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x0 = batch['clean'].to(device)   # (B,H,W)\n",
    "        cond = batch['noisy'].to(device) # (B,H,W)\n",
    "        if x0.ndim == 3:\n",
    "            x0 = x0.unsqueeze(1)\n",
    "        if cond.ndim == 3:\n",
    "            cond = cond.unsqueeze(1)\n",
    "\n",
    "        t = torch.randint(0, diffusion.T, (x0.shape[0],), device=device).long()\n",
    "\n",
    "        loss = diffusion.p_losses(x0, t, cond)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(diffusion, dataloader, device):\n",
    "    diffusion.eval()\n",
    "    val_loss = 0\n",
    "    gen_mse, gen_psnr = None, None\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x0 = batch['clean'].to(device)\n",
    "        cond = batch['noisy'].to(device)\n",
    "        if x0.ndim == 3:\n",
    "            x0 = x0.unsqueeze(1)\n",
    "        if cond.ndim == 3:\n",
    "            cond = cond.unsqueeze(1)\n",
    "\n",
    "        t = torch.randint(0, diffusion.T, (x0.shape[0],), device=device).long()\n",
    "        loss = diffusion.p_losses(x0, t, cond)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # For the first batch, generate a sample and compute metrics\n",
    "        if i == 0:\n",
    "            generated = diffusion.sample(cond, x0.shape)\n",
    "            mse_val = F.mse_loss(generated, x0).item()\n",
    "            psnr_val = 20 * math.log10(x0.max().item() / math.sqrt(mse_val)) if mse_val > 0 else 100\n",
    "            gen_mse, gen_psnr = mse_val, psnr_val\n",
    "\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    return avg_val_loss, gen_mse, gen_psnr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Create dataset and split\n",
    "full_dataset = RadarDataset(num_samples=100000)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)#batch_size=16\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False)#batch_size=16\n",
    "\n",
    "train_dataset_with_targets = RadarDataset(num_samples=102400, n_targets=8, random_n_targets=True)\n",
    "train_dataset_no_targets = RadarDataset(num_samples=10240, n_targets=0)\n",
    "train_dataset = ConcatDataset([train_dataset_with_targets, train_dataset_no_targets])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2,\n",
    "                            pin_memory=torch.cuda.is_available(), persistent_workers=True)\n",
    "\n",
    "# 3. Instantiate model and diffusion\n",
    "cond_unet = ConditionalUNet(in_channels=2, out_channels=1, time_emb_dim=32).to(device)\n",
    "cond_diffusion = ConditionalDiffusion(model=cond_unet, T=1000, beta_start=1e-4, beta_end=0.02).to(device)\n",
    "\n",
    "# 4. Optimizer\n",
    "optimizer = torch.optim.Adam(cond_diffusion.parameters(), lr=1e-5)\n",
    "\n",
    "# 5. Train Loop\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_psnrs = []\n",
    "val_mses = []\n",
    "\n",
    "num_epochs=1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(cond_diffusion, train_loader, optimizer, device)\n",
    "#     val_loss, gen_mse, gen_psnr = validate(cond_diffusion, val_loader, device)\n",
    "    \n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "#     if gen_psnr is not None:\n",
    "#         val_psnrs.append(gen_psnr)\n",
    "#     if gen_mse is not None:\n",
    "#         val_mses.append(gen_mse)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}\")\n",
    "#     if gen_mse is not None and gen_psnr is not None:\n",
    "#         print(f\"   [Generation Metrics] MSE: {gen_mse:.4f} | PSNR: {gen_psnr:.2f} dB\")\n",
    "\n",
    "#     # Save best model\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(cond_diffusion.state_dict(), \"best_cond_diffusion.pth\")\n",
    "#         print(\"   --> Best model saved.\")\n",
    "\n",
    "# # 6. Plot training curves\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(train_losses, label=\"Train Loss\")\n",
    "# plt.plot(val_losses, label=\"Val Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Training & Validation Loss\")\n",
    "# plt.show()\n",
    "\n",
    "# if len(val_psnrs) > 0:\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     plt.plot(val_psnrs, label=\"Val PSNR (dB)\")\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(\"PSNR\")\n",
    "#     plt.title(\"Validation PSNR over epochs\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1. Instantiate the same model and diffusion\n",
    "    cond_unet = ConditionalUNet(in_channels=2, out_channels=1, time_emb_dim=32).to(device)\n",
    "    cond_diffusion = ConditionalDiffusion(model=cond_unet, T=1000, beta_start=1e-4, beta_end=0.02).to(device)\n",
    "\n",
    "    # 2. Load the saved best checkpoint\n",
    "    checkpoint_path = \"best_cond_diffusion.pth\"\n",
    "    cond_diffusion.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    cond_diffusion.eval()\n",
    "\n",
    "    # 3. Get a sample from dataset (or your own custom input)\n",
    "    val_dataset = RadarDataset(num_samples=100, n_targets=8)\n",
    "\n",
    "\n",
    "\n",
    "    val_dataset_with_targets = RadarDataset(num_samples=2048, n_targets=8, random_n_targets=True,scnr=-10)\n",
    "    val_dataset_no_targets = RadarDataset(num_samples=2048, n_targets=0, scnr=-10)\n",
    "\n",
    "    val_dataset = ConcatDataset([val_dataset_with_targets, val_dataset_no_targets])\n",
    "\n",
    "\n",
    "    sample = val_dataset[0]\n",
    "    clean_img = sample['clean']  # shape (H,W)\n",
    "    noisy_img = sample['noisy']  # shape (H,W)\n",
    "\n",
    "    # 4. Prepare for the model\n",
    "    cond_img = noisy_img.unsqueeze(0).unsqueeze(0).to(device)  # (1,1,H,W)\n",
    "    shape = (1,1,clean_img.shape[0], clean_img.shape[1])       # same H,W\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_sample = cond_diffusion.sample(cond_img, shape)\n",
    "\n",
    "    # 5. Visualization\n",
    "    denoised_img = generated_sample.squeeze().cpu().numpy()\n",
    "    clean_np = clean_img.cpu().numpy()\n",
    "    noisy_np = noisy_img.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(clean_np, cmap='viridis')\n",
    "    axes[0].set_title(\"Clean RD Map\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(noisy_np, cmap='viridis')\n",
    "    axes[1].set_title(\"Noisy RD Map\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(denoised_img, cmap='viridis')\n",
    "    axes[2].set_title(\"Denoised RD Map (Conditional)\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
