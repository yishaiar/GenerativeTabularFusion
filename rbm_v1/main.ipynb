{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy-based model\n",
    "Energy-based models associate a scalar energy to each configuration of the variables of interest. Low energy is more desirable. The probability distribution based on an energy function can be defined as follows\n",
    "$$ \\Pr(x) = \\frac{\\exp (-E(x))}{Z}\\,,$$\n",
    "where $Z = \\sum_{x} \\exp (-E(x))$ denotes the normalization factor or **partition function**. \n",
    "\n",
    "### Restricted Boltzmann Machine\n",
    "\n",
    "Restricted Boltzmann Machine (RBM) has an efficient training algorithm. In order to increase the expressive power of the model, we do not observe the example $x$ fully, we also want to introduce some non-observed variables.  Consider an observed part $x$ and a hidden part $h$. We can then write:\n",
    "$$\\Pr(x) = \\sum_h \\frac{\\exp (-E(x, h))}{Z} \\,.$$\n",
    "\n",
    "In RBM, the energy function is defined as\n",
    "$$\n",
    "E(x, h) = -a^\\top x - b^\\top h - x^\\top W h \\,.\n",
    "$$\n",
    "\n",
    "To make RBM as an energy-based model, the free energy function is computed as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "F(x) &= -\\log \\sum_h \\exp (-E(x, h)) \\\\\n",
    "     &= -a^\\top x - \\sum_j \\log (1 + \\exp(W^{\\top}_jx + b_j))\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We have an tractable expression for the conditional probabilities\n",
    "$$\n",
    "\\Pr (h|x) = \\prod_i \\Pr (h_i | x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 42 set for CPU, CUDA (if available), and MPS (if available)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = '/Users/yishai/Desktop/GenerativeTabularFusion'\n",
    "import os\n",
    "os.chdir(base_dir)    \n",
    "from app.dotenv import base_dir, data_dir\n",
    "from app.utils import set_random_seed\n",
    "set_random_seed()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# from rbm import RBM\n",
    "from app.rbm.libs_v1 import train, show_and_save\n",
    "\n",
    "\n",
    "\n",
    "from app.rbm.model import RBM_v1\n",
    "\n",
    "RBM = RBM_v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "n_epochs: 10\n",
      "lr: 0.01\n",
      "n_hid: 128\n",
      "n_vis: 784\n"
     ]
    }
   ],
   "source": [
    "batch_size = int(os.getenv('batch_size'))# batch size\n",
    "n_epochs = int(os.getenv('n_epochs')) # number of epochs\n",
    "lr = float(os.getenv('lr')) # learning rate\n",
    "n_hid = int(os.getenv('n_hid')) # number of neurons in the hidden layer\n",
    "n_vis = int(os.getenv('n_vis')) # input size. i.e number of neurons in the visible layer\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'n_epochs: {n_epochs}')\n",
    "print(f'lr: {lr}')\n",
    "print(f'n_hid: {n_hid}')\n",
    "print(f'n_vis: {n_vis}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a RBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM_v1()\n"
     ]
    }
   ],
   "source": [
    "# create a Restricted Boltzmann Machine\n",
    "model = RBM(n_vis=n_vis, n_hid=n_hid, k=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  60000 test size:  10000\n",
      "batch size: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from app.load_data.MNIST import data_loaders\n",
    "train_loader, test_loader = data_loaders(batch_size,data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\t Loss=50.2742\n",
      "Epoch 1\t Loss=-0.0430\n",
      "Epoch 2\t Loss=-0.3284\n",
      "Epoch 3\t Loss=0.0846\n",
      "Epoch 4\t Loss=0.3507\n",
      "Epoch 5\t Loss=0.4989\n",
      "Epoch 6\t Loss=0.6497\n",
      "Epoch 7\t Loss=0.9204\n",
      "Epoch 8\t Loss=1.0450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Epoch 0\t Loss=50.2742\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Epoch 1\t Loss=-0.0430\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Epoch 2\t Loss=-0.3284\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Epoch 8\t Loss=1.0450\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Epoch 9\t Loss=1.1167\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/app/rbm/libs_v1.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, n_epochs, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 42\u001b[0m     v, v_gibbs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfree_energy(v) \u001b[38;5;241m-\u001b[39m model\u001b[38;5;241m.\u001b[39mfree_energy(v_gibbs)\n\u001b[1;32m     44\u001b[0m     loss_\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/app/rbm/model.py:88\u001b[0m, in \u001b[0;36mRBM_v1.forward\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m     86\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisible_to_hidden(v)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[0;32m---> 88\u001b[0m     v_gibb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_to_visible\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisible_to_hidden(v_gibb)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v, v_gibb\n",
      "File \u001b[0;32m~/Desktop/GenerativeTabularFusion/app/rbm/model.py:53\u001b[0m, in \u001b[0;36mRBM_v1.hidden_to_visible\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Conditional sampling a visible variable given a hidden variable.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(F\u001b[38;5;241m.\u001b[39mlinear(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mt(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv))\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, n_epochs=n_epochs, lr=lr)\n",
    "\n",
    "# Epoch 0\t Loss=50.2742\n",
    "# Epoch 1\t Loss=-0.0430\n",
    "# Epoch 2\t Loss=-0.3284\n",
    "# Epoch 3\t Loss=0.0846\n",
    "# Epoch 4\t Loss=0.3507\n",
    "# Epoch 5\t Loss=0.4989\n",
    "# Epoch 6\t Loss=0.6497\n",
    "# Epoch 7\t Loss=0.9204\n",
    "# Epoch 8\t Loss=1.0450\n",
    "# Epoch 9\t Loss=1.1167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(train_loader))[0]\n",
    "v, v_gibbs = model(images.view(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the real images\n",
    "os.chdir('./rbm_v1')\n",
    "show_and_save(make_grid(v.view(batch_size, 1, 28, 28).data), 'output/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the generated images\n",
    "show_and_save(make_grid(v_gibbs.view(batch_size, 1, 28, 28).data), 'output/fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How one image is factorized through the hidden variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 4\n",
    "kth = 18\n",
    "d = images[kth:kth+1]\n",
    "\n",
    "V = torch.sigmoid(F.linear(d.view(1, -1), model.W, model.h))\n",
    "v, o = torch.sort(V.view(-1))\n",
    "\n",
    "fig, ax = plt.subplots(1, n_sample + 1, figsize=(3*(1 + n_sample),3))\n",
    "ax[0].imshow(d.view(28, 28).numpy(), cmap='gray')\n",
    "ax[0].set_title('Original image')\n",
    "\n",
    "for k, i in enumerate(o[-n_sample:].numpy()):\n",
    "    f = model.W[i].view(28, 28).data.numpy()\n",
    "    ax[k + 1].imshow(f, cmap='gray')\n",
    "    ax[k + 1].set_title('p=%.2f'% V[0][i].item())\n",
    "    \n",
    "plt.savefig('output/factor.png', dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
